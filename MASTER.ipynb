{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "d7cbc32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import math\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from matplotlib import pyplot\n",
    "from scipy.stats import uniform, randint\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Model and performance evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from joblib import dump, load\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score \n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import auc, average_precision_score, precision_recall_curve\n",
    "\n",
    "# Hyperparameter tuninghCV\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from hyperopt import tpe, STATUS_OK, Trials, hp, fmin, STATUS_OK, space_eval\n",
    "from xgboost import plot_importance\n",
    "\n",
    "# Data imputation\n",
    "from imblearn.over_sampling import SMOTE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09f2a8f",
   "metadata": {},
   "source": [
    "## 1. Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab85f41b",
   "metadata": {},
   "source": [
    "#### 1) Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "784aba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clf_eval(y_test, pred=None, pred_proba=None):\n",
    "    confusion = confusion_matrix(y_test, pred)\n",
    "    accuracy = accuracy_score(y_test , pred)\n",
    "    precision = precision_score(y_test , pred)\n",
    "    recall = recall_score(y_test , pred)\n",
    "    f1 = f1_score(y_test,pred)\n",
    "    roc_auc = roc_auc_score(y_test, pred_proba)\n",
    "    precision_, recall_, thr = precision_recall_curve(y_test, pred_proba)\n",
    "    pr_auc = auc(recall_, precision_)\n",
    "    result = '{0:.4f}, {1:.4f}, {2:.4f}, {3:.4f}, {4:.4f}, {5:.4f}'.format(accuracy, precision, recall, f1, roc_auc, pr_auc)\n",
    "    return confusion, result \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca25e9c",
   "metadata": {},
   "source": [
    "#### 2) Plot AUROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "bfba059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_curve_plot(y_test, opted_predict_prob,label_name):\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, opted_predict_prob)\n",
    "\n",
    "    plt.plot(fpr, tpr, label = label_name)\n",
    "    plt.plot([0,1], 'k--')\n",
    "    plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')\n",
    "    plt.title(\"AUROC Curves\")\n",
    "    plt.legend(fontsize=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2edb9c",
   "metadata": {},
   "source": [
    "#### 3) Plot AUPRC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "3ea1f838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pr_curve(recall, precision,label_name):\n",
    "    pyplot.plot([0, 1], 'k--')\n",
    "    pyplot.plot(recall, precision, label=label_name)\n",
    "    # axis labels\n",
    "    pyplot.xlabel('Recall')\n",
    "    pyplot.ylabel('Precision')\n",
    "    plt.title(\"AUPRC\")\n",
    "    pyplot.legend(fontsize=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e67af3e",
   "metadata": {},
   "source": [
    "#### 4) Preprocess_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "fb31626b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(input, output):\n",
    "\n",
    "    features = pd.read_excel(input)\n",
    "    label = pd.read_excel(output)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, label, \n",
    "                                                    test_size=0.2, random_state=1)\n",
    "    \n",
    "    y_train = np.ravel(y_train)\n",
    "    y_test = np.ravel(y_test)\n",
    "    \n",
    "    # imputating train_data\n",
    "    smote = SMOTE(random_state=0)\n",
    "    X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16f47ff",
   "metadata": {},
   "source": [
    "#### 5) Plot_curve\n",
    "version 1과 version 2 중에 하나만 실행하기. \n",
    "실행 후 각 모델별 함수('## Ploting curve' 확인)에서 실행될 수 있도록 주석 확인."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "0df55c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 1_input 또는 output 고정하여 비교할 경우 실행\n",
    "def plot_curve(input, output, curve, y_test, opted_predict_prob):\n",
    "    \n",
    "    # set label\n",
    "    in_ = re.sub(r'[^0-9]', '', input) \n",
    "    out_ = re.sub(r'[^0-9]', '', output)\n",
    "    in_ = int(in_)\n",
    "      \n",
    "    # plot AUCROC curve\n",
    "    if curve == 'auroc':\n",
    "        roc_auc = round(roc_auc_score(y_test, opted_predict_prob), 3)\n",
    "        label = f\"{in_-2}~{in_} Hours after ICU admission, {curve.upper()}: {roc_auc}\"\n",
    "        roc_curve_plot(y_test, opted_predict_prob, label_name = label)\n",
    "        \n",
    "    # plot AUPRC curve\n",
    "    elif curve == 'auprc':\n",
    "        precision, recall, thresholds = precision_recall_curve(y_test, opted_predict_prob)\n",
    "        pr_auc = round(auc(recall, precision),3)\n",
    "        label = f\"{in_-2}~{in_} Hours after ICU admission, {curve.upper()}: {pr_auc}\"\n",
    "        plot_pr_curve(recall, precision, label_name=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "3fcedc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 2_MODEL별로 비교할 경우 실행\n",
    "def plot_curve_(input, output, curve, y_test, opted_predict_prob, model_name):\n",
    "    \n",
    "    # set label\n",
    "    in_ = re.sub(r'[^0-9]', '', input) \n",
    "    out_ = re.sub(r'[^0-9]', '', output)\n",
    "      \n",
    "    # plot AUROC curve\n",
    "    if curve == 'auroc':\n",
    "        roc_auc = round(roc_auc_score(y_test, opted_predict_prob), 3)\n",
    "        label = f\"{model_name}, {curve.upper()}: {roc_auc}\"\n",
    "        roc_curve_plot(y_test, opted_predict_prob, label_name = label)\n",
    "        \n",
    "    # plot AUPRC curve\n",
    "    elif curve == 'auprc':\n",
    "        precision, recall, thresholds = precision_recall_curve(y_test, opted_predict_prob)\n",
    "        pr_auc = round(auc(recall, precision),3)\n",
    "        label = f\"{model_name}, {curve.upper()}: {pr_auc}\"\n",
    "        plot_pr_curve(recall, precision, label_name=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4006f3e4",
   "metadata": {},
   "source": [
    "#### 6) Train & Calibrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "ad1fd50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_calibrate_model(name, model, calibration, param, X_train, y_train, X_test, y_test, input, output):    \n",
    "    in_ = re.sub(r'[^0-9]', '', input)\n",
    "    out_ = re.sub(r'[^0-9]', '', output)\n",
    "\n",
    "    # Grid search CV\n",
    "    if calibration == 'grid' :\n",
    "        \n",
    "        scoring = ['roc_auc']\n",
    "   \n",
    "        kfold = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=1)\n",
    "\n",
    "        grid_search = GridSearchCV(estimator=model, \n",
    "                           param_grid=param, \n",
    "                           scoring= scoring, \n",
    "                           refit='roc_auc', \n",
    "                           n_jobs=-1, \n",
    "                           cv=kfold, \n",
    "                           verbose=0)\n",
    "    \n",
    "        grid_result = grid_search.fit(X_train, y_train)\n",
    "    \n",
    "        model = grid_search.best_estimator_\n",
    "\n",
    "        opted_predict = grid_search.predict(X_test)\n",
    "    \n",
    "        # Get predicted probabilities\n",
    "        opted_predict_prob = grid_search.predict_proba(X_test)[:,1]\n",
    "  \n",
    "    # Randomized search CV\n",
    "    elif calibration == 'random':\n",
    "\n",
    "        scoring = ['roc_auc']\n",
    "  \n",
    "        kfold = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=0)\n",
    "\n",
    "        random_search = RandomizedSearchCV(estimator=model, \n",
    "                           param_distributions=param, \n",
    "                           n_iter=48,\n",
    "                           scoring=scoring, \n",
    "                           refit='roc_auc', \n",
    "                           n_jobs=-1, \n",
    "                           cv=kfold, \n",
    "                           verbose=0)\n",
    "  \n",
    "        random_result = random_search.fit(X_train, y_train)\n",
    "    \n",
    "        dump(random_search, f'./trained/{name}_matrix_trained_{calibration }_{in_}hr_{out_}hr.joblib') \n",
    "        print(f\"save matrix: {name}_matrix_trained_{calibration }_{in_}hr_{out_}hr.joblib\")\n",
    "\n",
    "        opted_predict = random_search.predict(X_test)\n",
    "        opted_predict_prob = random_search.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    return opted_predict, opted_predict_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d77a9a",
   "metadata": {},
   "source": [
    "## 2. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32046a9",
   "metadata": {},
   "source": [
    "#### 1) ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "514fe16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ANN_model(input, output, n_fold, calibration, curve):\n",
    "    \n",
    "    ## processing data for training/test\n",
    "    features = pd.read_excel(input)\n",
    "    label = pd.read_excel(output)\n",
    "    \n",
    "    len_features = int(features.shape[1])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, label, test_size=0.2, random_state=42)\n",
    "    \n",
    "    y_train = np.ravel(y_train)\n",
    "    y_test = np.ravel(y_test)\n",
    "    \n",
    "    smote = SMOTE(random_state=0)\n",
    "    X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    \n",
    "    ## model \n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    model.add(keras.layers.InputLayer(input_shape=(len_features,)))\n",
    "    \n",
    "    model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    model = KerasClassifier(model)\n",
    "\n",
    "    ## optimizing hyper-param for model\n",
    "    \n",
    "    # # Grid search CV\n",
    "    if calibration == 'grid' :\n",
    "        param = {'batch_size': [16, 32, 64, 128],\n",
    "                 'epochs': [10, 20]\n",
    "                }\n",
    "\n",
    "        grid_search = GridSearchCV(estimator=model, param_grid=param, scoring='accuracy', cv=3)\n",
    "\n",
    "        grid_result = grid_search.fit(X_train, y_train)\n",
    "\n",
    "        best_model = grid_search.best_estimator_\n",
    "        opted_predict = best_model.predict(X_test)\n",
    "        \n",
    "        opted_predict_prob = grid_search.predict_proba(X_test)[:,1]\n",
    "    \n",
    "        \n",
    "  \n",
    "    # Randomized search CV\n",
    "    elif calibration == 'random':\n",
    "        param = {'batch_size': [16, 32, 64, 128],\n",
    "                 'epochs': [10, 20]\n",
    "                }\n",
    "        \n",
    "         # Set up score\n",
    "        scoring = ['accuracy']\n",
    "   \n",
    "        kfold = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=1)\n",
    "\n",
    "\n",
    "        random_search = RandomizedSearchCV(estimator=model, \n",
    "                                   param_distributions=param, \n",
    "                                   scoring=scoring, \n",
    "                                   cv=kfold,\n",
    "                                   refit='accuracy', \n",
    "                                   n_jobs=-1, \n",
    "                                   verbose=0)\n",
    "\n",
    "        random_result = random_search.fit(X_train, y_train)\n",
    "\n",
    "        best_model = random_search.best_estimator_\n",
    "        opted_predict = best_model.predict(X_test)\n",
    "        \n",
    "        opted_predict_prob = random_search.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    ## Ploting curve\n",
    "    plot_curve(input, output, curve, y_test, opted_predict_prob)\n",
    "    \n",
    "    ## Get performance metrics\n",
    "    temp_comfusion, temp_result = get_clf_eval(y_test , opted_predict, opted_predict_prob)\n",
    "\n",
    "    return temp_comfusion, temp_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635d220e",
   "metadata": {},
   "source": [
    "#### 2) Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "284b621a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR_model(name, input, output, n_fold, calibration, curve):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = preprocess_data(input, output)\n",
    "\n",
    "    \n",
    "    ## model \n",
    "    model = LogisticRegression(random_state=1, max_iter=2000)\n",
    "\n",
    "    ## optimizing hyper-param for model \n",
    "    if calibration == 'grid' :\n",
    "        param = {'C':[0.001,0.01,0.1,1,10], \n",
    "                 'penalty':['l1', 'l2']  \n",
    "                }\n",
    "    elif calibration == 'random':\n",
    "        \n",
    "        param = {'C':[0.001,0.01,0.1,1,10],\n",
    "                 'penalty': ['l1','l2'],\n",
    "                }\n",
    "        \n",
    "    opted_predict, opted_predict_prob = train_and_calibrate_model(name, model, calibration, param, X_train, y_train, X_test, y_test, input, output)\n",
    "    \n",
    "    ## Ploting curve\n",
    "    plot_curve(input, output, curve, y_test, opted_predict_prob)\n",
    "    \n",
    "    ## Get performance metrics\n",
    "    temp_comfusion, temp_result = get_clf_eval(y_test , opted_predict, opted_predict_prob)\n",
    "\n",
    "    return temp_comfusion, temp_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a15e12f",
   "metadata": {},
   "source": [
    "#### 3) KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "85f65707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_model(name, input, output, n_fold, calibration, curve):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = preprocess_data(input, output)\n",
    "    \n",
    "    ## model \n",
    "    model = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "    ## optimizing hyper-param for model \n",
    "    if calibration == 'grid' :\n",
    "        param = {'n_neighbors' : list(range(1,30)),\n",
    "                 'weights' : [\"uniform\", \"distance\"],\n",
    "                 'metric' : ['euclidean', 'manhattan', 'minkowski'],\n",
    "                 'leaf_size' : range(1, 50, 5)\n",
    "                }\n",
    "        \n",
    "    elif calibration == 'random':\n",
    "  \n",
    "        param = {'n_neighbors' : list(range(1,30)),\n",
    "                 'weights' : [\"uniform\", \"distance\"],\n",
    "                 'metric' : ['euclidean', 'manhattan', 'minkowski'],\n",
    "                 'leaf_size' : range(1, 50, 5)\n",
    "                }\n",
    "\n",
    "    opted_predict, opted_predict_prob = train_and_calibrate_model(name, model, calibration, param, X_train, y_train, X_test, y_test, input, output)\n",
    "    \n",
    "    ## Ploting curve\n",
    "    plot_curve(input, output, curve, y_test, opted_predict_prob)\n",
    "    \n",
    "    ## Get performance metrics\n",
    "    temp_comfusion, temp_result = get_clf_eval(y_test , opted_predict, opted_predict_prob)\n",
    "\n",
    "    return temp_comfusion, temp_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d1f403",
   "metadata": {},
   "source": [
    "#### 4) Logistic Regeression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "59951b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DT_model(name, input, output, n_fold, calibration, curve):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = preprocess_data(input, output)\n",
    "    \n",
    "    ## model \n",
    "    model = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "    ## optimizing hyper-param for model\n",
    "    if calibration == 'grid' :\n",
    "        param = {'min_impurity_decrease': [0.001, 0.01, 0.1],\n",
    "                 'max_depth': range(5, 20, 1),\n",
    "                 'min_samples_split': range(2, 100, 10)\n",
    "                }\n",
    "    elif calibration == 'random':\n",
    "  \n",
    "        param = {'min_impurity_decrease': [0.001, 0.01, 0.1],\n",
    "                 'max_depth': range(5, 20, 1),\n",
    "                 'min_samples_split': range(2, 100, 10)\n",
    "                }\n",
    "\n",
    "    opted_predict, opted_predict_prob = train_and_calibrate_model(name, model, calibration, param, X_train, y_train, X_test, y_test, input, output)\n",
    "    \n",
    "    ## Ploting curve\n",
    "    plot_curve(input, output, curve, y_test, opted_predict_prob)\n",
    "    \n",
    "    ## Get performance metrics\n",
    "    temp_comfusion, temp_result = get_clf_eval(y_test , opted_predict, opted_predict_prob)\n",
    "\n",
    "    return temp_comfusion, temp_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb2fe78",
   "metadata": {},
   "source": [
    "#### 5) Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "17683845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_model(name, input, output, n_fold, calibration, curve):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = preprocess_data(input, output)\n",
    "    \n",
    "    ## model \n",
    "    model = RandomForestClassifier(n_jobs=-1, random_state=1)\n",
    "\n",
    "    ## optimizing hyper-param for model\n",
    "    if calibration == 'grid' :\n",
    "    \n",
    "        param = {'n_estimators' : [10, 100],\n",
    "                 'max_depth' : [6, 8, 10, 12],\n",
    "                 'min_samples_leaf' : [8, 12, 16, 20],\n",
    "                 'min_samples_split' : [8, 12, 16, 20]\n",
    "                }\n",
    "        \n",
    "    elif calibration == 'random':\n",
    "  \n",
    "        param = {'n_estimators' : [10, 100],\n",
    "                 'max_depth' : [6, 8, 10, 12],\n",
    "                 'min_samples_leaf' : [8, 12, 16, 20],\n",
    "                 'min_samples_split' : [8, 12, 16, 20]\n",
    "                }\n",
    "\n",
    "    opted_predict, opted_predict_prob = train_and_calibrate_model(name, model, calibration, param, X_train, y_train, X_test, y_test, input, output)\n",
    "    \n",
    "    ## Ploting curve\n",
    "    plot_curve(input, output, curve, y_test, opted_predict_prob)\n",
    "    \n",
    "    ## Get performance metrics\n",
    "    temp_comfusion, temp_result = get_clf_eval(y_test , opted_predict, opted_predict_prob)\n",
    "\n",
    "    return temp_comfusion, temp_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eec655d",
   "metadata": {},
   "source": [
    "#### 6) SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "9e1f791c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM_model(name, input, output, n_fold, calibration, curve):\n",
    "    \n",
    "    ## processing data for training/test\n",
    "    X_train, X_test, y_train, y_test = preprocess_data(input, output)\n",
    "\n",
    "    \n",
    "    ## model \n",
    "    model = SVC(random_state=0, probability=True)\n",
    "\n",
    "    ## optimizing hyper-param for model\n",
    "    \n",
    "    # grid search \n",
    "    if calibration == 'grid' :\n",
    "        param = {'C': [0.1,1, 10, 100], \n",
    "         'kernel': ['poly','sigmoid','linear','rbf'],\n",
    "         'gamma': [1,0.1,0.01,0.001]}\n",
    "\n",
    "    # random search\n",
    "    elif calibration == 'random':\n",
    "  \n",
    "        param = {'C': [0.1,1, 10, 100], \n",
    "         'kernel': ['poly','sigmoid','linear','rbf'],\n",
    "         'gamma': [1,0.1,0.01,0.001]}\n",
    "    \n",
    "    opted_predict, opted_predict_prob = train_and_calibrate_model(name, model, calibration, param, X_train, y_train, X_test, y_test, input, output)\n",
    "\n",
    "    ## Ploting curve\n",
    "    plot_curve(input, output, curve, y_test, opted_predict_prob)\n",
    "    \n",
    "    ## Get performance metrics\n",
    "    temp_comfusion, temp_result = get_clf_eval(y_test , opted_predict, opted_predict_prob)\n",
    "\n",
    "\n",
    "    return temp_comfusion, temp_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca502af",
   "metadata": {},
   "source": [
    "#### 7) XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "f01d79a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGB_model(name, input, output, n_fold, calibration, curve):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = preprocess_data(input, output)\n",
    "    \n",
    "    ## model \n",
    "    model = XGBClassifier(booster='gbtree', objective='binary:logistic')\n",
    "\n",
    "    ## optimizing hyper-param for model\n",
    "    if calibration == 'grid' :\n",
    "        param = {'n_estimators': [100,200,400,800,1000],\n",
    "                 'learning_rate': [0.01,0.05,0.1,0.2,0.3,0.4,0.5],\n",
    "                 'gamma': [0,0.01,0.1,0.5,1,2],\n",
    "                 'min_child_weight':[1,2,3,4,5]\n",
    "                }\n",
    "    # random search\n",
    "    elif calibration == 'random':\n",
    "  \n",
    "        param = {'n_estimators': [100,200,400,800,1000],\n",
    "                 'learning_rate': [0.01,0.05,0.1,0.2,0.3,0.4,0.5],\n",
    "                 'gamma': [0,0.01,0.1,0.5,1,2],\n",
    "                 'min_child_weight':[1,2,3,4,5]\n",
    "                }\n",
    "\n",
    "    opted_predict, opted_predict_prob = train_and_calibrate_model(name, model, calibration, param, X_train, y_train, X_test, y_test, input, output)\n",
    "    \n",
    "    ## Ploting curve\n",
    "    plot_curve(input, output, curve, y_test, opted_predict_prob)\n",
    "    \n",
    "    ## Get performance metrics\n",
    "    temp_comfusion, temp_result = get_clf_eval(y_test , opted_predict, opted_predict_prob)\n",
    "\n",
    "    return temp_comfusion, temp_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91cd1b4",
   "metadata": {},
   "source": [
    "## 3. Execute model\n",
    "\n",
    "아래 세 함수 중 하나만 실행하기."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381e6989",
   "metadata": {},
   "source": [
    "#### 1) Input 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d34de819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute(dir_input, dir_output, n_fold, imputation, calibration, curve, name):    \n",
    "    i = 0\n",
    "    total = len(dir_input)*len(dir_output)\n",
    "    df_result = pd.DataFrame(columns = [\"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"AUROC\", \"AUPRC\"])\n",
    "    \n",
    "    models = {'ANN': ANN_model,\n",
    "              'DT': DT_model,\n",
    "              'KNN': KNN_model,\n",
    "              'LR': LR_model,\n",
    "              'RF': RF_model,\n",
    "              'SVM': SVM_model,\n",
    "              'XGB': XGB_model\n",
    "             }\n",
    "    \n",
    "    model = models[name]\n",
    "    \n",
    "    for input in dir_input:\n",
    "        in_ = re.sub(r'[^0-9]', '', input)\n",
    "        idx = {'Accuracy': f'{in_}hr'}\n",
    "        df_result = df_result.append(idx, ignore_index=True)\n",
    "  \n",
    "        for output in dir_output:\n",
    "            print(f\"{i/total*100}% processing..\")\n",
    "        \n",
    "            ## Run\n",
    "            confusion, result = model(name, input, output, n_fold, calibration, curve)\n",
    "        \n",
    "            ## post-process results from model\n",
    "            dic = {}\n",
    "            temp = result.split(', ')\n",
    "\n",
    "            index = 0\n",
    "            for key in df_result.keys():\n",
    "                dic[key] = temp[index]\n",
    "                index += 1\n",
    "\n",
    "            df_result = df_result.append(dic, ignore_index=True)\n",
    "            plt.savefig(f'./result_graph/{name}_{imputation}_{curve}_curve_{out_}hr.png', dpi=300)\n",
    "            i = i + 1\n",
    "        \n",
    "        plt.cla()\n",
    "\n",
    "    # export    \n",
    "    df_result.to_csv(f'./result_csv/result_{name}_{calibration}_{imputation}_prolonged.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910a9487",
   "metadata": {},
   "source": [
    "#### 2) Output 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "19554215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute(dir_input, dir_output, n_fold, imputation, calibration, curve, name):    \n",
    "    i = 0\n",
    "    total = len(dir_input)*len(dir_output)\n",
    "    df_result = pd.DataFrame(columns = [\"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"AUROC\", \"AUPRC\"])\n",
    "    \n",
    "    models = {'ANN': ANN_model,\n",
    "              'DT': DT_model,\n",
    "              'KNN': KNN_model,\n",
    "              'LR': LR_model,\n",
    "              'RF': RF_model,\n",
    "              'SVM': SVM_model,\n",
    "              'XGB': XGB_model\n",
    "             }\n",
    "    \n",
    "    model = models[name]\n",
    "    \n",
    "    for output in dir_output:\n",
    "        out_ = re.sub(r'[^0-9]', '', output)\n",
    "        idx = {'Accuracy': f'{out_}hr'}\n",
    "        df_result = df_result.append(idx, ignore_index=True)\n",
    "    \n",
    "        for input in dir_input:\n",
    "            \n",
    "            in_ = re.sub(r'[^0-9]', '', input)\n",
    "            print(f\"{i/total*100}% processing..\")\n",
    "        \n",
    "            ## Run\n",
    "            confusion, result = model(name, input, output, n_fold, calibration, curve)\n",
    "        \n",
    "            ## post-process results from model\n",
    "            dic = {}\n",
    "            temp = result.split(', ')\n",
    "\n",
    "            index = 0\n",
    "            for key in df_result.keys():\n",
    "                dic[key] = temp[index]\n",
    "                index += 1\n",
    "\n",
    "            df_result = df_result.append(dic, ignore_index=True)\n",
    "            plt.savefig(f'./result_graph/{name}_{imputation}_{curve}_curve_{out_}hr.png', dpi=300)\n",
    "            i = i + 1\n",
    "        \n",
    "        plt.cla()\n",
    "        \n",
    "        # export    \n",
    "        df_result.to_csv(f'./result_csv/result_{name}_{calibration}_{imputation}_prolonged.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97abdcfb",
   "metadata": {},
   "source": [
    "#### 3) 모델별 그래프 (input, output 고정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "daf9c9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute(input_path, output_path, n_fold, imputation, calibration, curve, list_models):    \n",
    "     \n",
    "    models = {'ANN': ANN_model,\n",
    "              'DT': DT_model,\n",
    "              'KNN': KNN_model,\n",
    "              'LR': LR_model,\n",
    "              'RF': RF_model,\n",
    "              'SVM': SVM_model,\n",
    "              'XGB': XGB_model\n",
    "             }\n",
    "    \n",
    "    \n",
    "    in_ = re.sub(r'[^0-9]', '', input_path)\n",
    "    out_ = re.sub(r'[^0-9]', '', output_path)\n",
    "    \n",
    "    ANN_model(input_path, output_path, 'random', curve)\n",
    "        \n",
    "    plt.savefig(f'./result_graph/ALL_{imputation}_{curve}_curve_{in_}hr.png', dpi=300)\n",
    "    \n",
    "    total = len(list_models)\n",
    "    i = 0\n",
    "    \n",
    "    for name in list_models:\n",
    "        if name == 'ANN':\n",
    "            break\n",
    "        \n",
    "        model = models[name]\n",
    "        \n",
    "        print(f\"{i/total*100}% processing..\")\n",
    "        \n",
    "        ## Run\n",
    "        confusion, result = model(name, input_path, output_path, n_fold, calibration, curve)\n",
    "    \n",
    "        plt.savefig(f'./result_graph/ALL_{imputation}_{curve}_curve_{in_}hr_{out_}hr.png', dpi=300)\n",
    "        \n",
    "        i = i + 1\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b81d27",
   "metadata": {},
   "source": [
    "#### 4) Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "8dbd6032",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dir_input = [\n",
    "    './df_input/df_2hr.xlsx',\n",
    "    './df_input/df_4hr.xlsx',\n",
    "    './df_input/df_6hr.xlsx',\n",
    "    './df_input/df_8hr.xlsx',\n",
    "    './df_input/df_10hr.xlsx',\n",
    "    './df_input/df_12hr.xlsx'\n",
    "]\n",
    "\n",
    "dir_output = [\n",
    "      './df_output/df_categorical_output_hr/df_categorical_output_24hr.xlsx',\n",
    "     './df_output/df_categorical_output_hr/df_categorical_output_36hr.xlsx',\n",
    "    './df_output/df_categorical_output_hr/df_categorical_output_48hr.xlsx',\n",
    "    './df_output/df_categorical_output_hr/df_categorical_output_60hr.xlsx',\n",
    "    './df_output/df_categorical_output_hr/df_categorical_output_72hr.xlsx',\n",
    "    './df_output/df_categorical_output_hr/df_categorical_output_120hr.xlsx',\n",
    "    './df_output/df_categorical_output_hr/df_categorical_output_168hr.xlsx',\n",
    "    './df_output/df_categorical_output_hr/df_categorical_output_336hr.xlsx'\n",
    " ]  \n",
    "\n",
    "n_fold = 5\n",
    "imputation = 'smote'\n",
    "calibration = 'random'\n",
    "curves = ['auroc'] # auroc: AUROC/ auprc: AUPRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ff4c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델별로 성능을 비교할 경우에 사용\n",
    "\n",
    "dir_input = './df_input/df_12hr.xlsx'\n",
    "dir_output = './df_output/df_categorical_output_hr/df_categorical_output_60hr.xlsx'\n",
    "\n",
    "models = ['ANN','DT','KNN','LR','RF','SVM','XGB']\n",
    "n_fold = 5\n",
    "imputation = 'smote'\n",
    "calibration = 'random'\n",
    "curves = ['auroc', 'auprc'] # auroc: AUROC/ auprc: AUPRC\n",
    "\n",
    "for curve in curves:\n",
    "    execute(dir_input, dir_output, n_fold, imputation, calibration, curve, models)\n",
    "    plt.cla()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb44862",
   "metadata": {},
   "source": [
    "#### 5) Execute single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c7a1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ['SVM'] #'LR','DT','RF','XGB','KNN','SVM'\n",
    "curves = ['auroc', 'auprc'] # auroc: AUROC/ auprc: AUPRC\n",
    "\n",
    "for curve in curves:\n",
    "    execute(dir_input, dir_output, n_fold, imputation, calibration, curve, model)\n",
    "    plt.cla()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273e3d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ANN single execution\n",
    "\n",
    "curves = ['auroc', 'auprc'] # auroc: AUROC/ auprc: AUPRC\n",
    "\n",
    "for curve in curves:\n",
    "    i = 0\n",
    "    total = len(dir_input)*len(dir_output)\n",
    "    df_result = pd.DataFrame(columns = [\"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"ROC AUC\", \"PR AUC\"])\n",
    "    \n",
    "    for output in dir_output:\n",
    "        out_ = re.sub(r'[^0-9]', '', output)\n",
    "        idx = {'Accuracy': f'{out_}hr'}\n",
    "        df_result = df_result.append(idx, ignore_index=True)\n",
    "    \n",
    "        for input in dir_input:\n",
    "            \n",
    "            in_ = re.sub(r'[^0-9]', '', input)\n",
    "            print(f\"{i/total*100}% processing..\")\n",
    "\n",
    "        \n",
    "            ## Run\n",
    "            confusion, result = ANN_model(input, output, n_fold, calibration, curve)\n",
    "        \n",
    "            ## post-process results from model\n",
    "            dic = {}\n",
    "            temp = result.split(', ')\n",
    "\n",
    "            index = 0\n",
    "            for key in df_result.keys():\n",
    "                dic[key] = temp[index]\n",
    "                index += 1\n",
    "\n",
    "            df_result = df_result.append(dic, ignore_index=True)\n",
    "            plt.savefig(f'./result_graph/ANN_{imputation}_{curve}_curve_{out_}hr.png', dpi=300)\n",
    "            i = i + 1\n",
    "        \n",
    "        plt.cla()\n",
    "\n",
    "    # export    \n",
    "    df_result.to_csv(f'./result_csv/result_ANN_{calibration}_{imputation}_prolonged.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68380570",
   "metadata": {},
   "source": [
    "#### 6) Execute multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d465e4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['ANN','DT','KNN','LR','RF','SVM','XGB']\n",
    "curves = ['auroc', 'auprc'] # auroc: AUROC/ auprc: AUPRC\n",
    "\n",
    "for model in models:\n",
    "    for curve in curves:\n",
    "        execute(dir_input, dir_output, n_fold, imputation, calibration, curve, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
